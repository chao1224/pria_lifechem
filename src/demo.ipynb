{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from function import *\n",
    "from data_preparation import *\n",
    "from evaluation import *\n",
    "\n",
    "from openpyxl import Workbook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-prepare Data (Data Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform_data(output_file_name='../dataset/keck_complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecule                   object\n",
      "SMILES                     object\n",
      "Fingerprints               object\n",
      "Keck_Pria_AS_Retest         int64\n",
      "Keck_Pria_FP_data           int64\n",
      "Keck_Pria_Continuous      float64\n",
      "Keck_RMI_cdd              float64\n",
      "FP counts % inhibition    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "complete_df = pd.read_csv('../dataset/keck_complete.csv')\n",
    "\n",
    "print complete_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pria retest active: 79\tpria fp active: 24\trmi cdd active: 230\n",
      "{0: 72094, 1: 325, 2: 4}\n",
      "\n",
      "retest: 0, fp: 0, rmi: 0 \t--- 49489\n",
      "retest: 0, fp: 0, rmi: nan \t--- 22605\n",
      "retest: 0, fp: 1, rmi: 0 \t--- 19\n",
      "retest: 0, fp: 1, rmi: 1 \t--- 3\n",
      "retest: 1, fp: 0, rmi: 0 \t--- 58\n",
      "retest: 1, fp: 0, rmi: nan \t--- 20\n",
      "retest: 0, fp: 1, rmi: nan \t--- 1\n",
      "retest: 0, fp: 0, rmi: 1 \t--- 227\n",
      "retest: 1, fp: 1, rmi: nan \t--- 1\n"
     ]
    }
   ],
   "source": [
    "cnt_pria_retest = 0\n",
    "cnt_pria_fp = 0\n",
    "cnt_rmi_cdd = 0\n",
    "\n",
    "cnt_dict = {}\n",
    "for ix, row in complete_df.iterrows():\n",
    "    cnt = 0\n",
    "    if row['Keck_Pria_AS_Retest'] == 1:\n",
    "        cnt_pria_retest += 1\n",
    "        cnt += 1\n",
    "    if row['Keck_Pria_FP_data'] == 1:\n",
    "        cnt_pria_fp += 1\n",
    "        cnt += 1\n",
    "    if row['Keck_RMI_cdd'] == 1:\n",
    "        cnt_rmi_cdd += 1\n",
    "        cnt += 1\n",
    "    if cnt not in cnt_dict.keys():\n",
    "        cnt_dict[cnt] = 0\n",
    "    cnt_dict[cnt] += 1\n",
    "\n",
    "print 'pria retest active: {}\\tpria fp active: {}\\trmi cdd active: {}'.format(cnt_pria_retest, cnt_pria_fp, cnt_rmi_cdd)\n",
    "print cnt_dict\n",
    "\n",
    "\n",
    "print\n",
    "analysis(complete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 3\n",
    "directory = '../dataset/fixed_dataset/fold_{}/'.format(k)\n",
    "file_list = []\n",
    "for i in range(k):\n",
    "    file_list.append('file_{}.csv'.format(i))\n",
    "# greedy_multi_splitting(complete_df, k, directory=directory, file_list=file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "directory = '../dataset/fixed_dataset/fold_{}/'.format(k)\n",
    "file_list = []\n",
    "for i in range(k):\n",
    "    file_list.append('file_{}.csv'.format(i))\n",
    "# greedy_multi_splitting(complete_df, k, directory=directory, file_list=file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "directory = '../dataset/fixed_dataset/fold_{}/'.format(k)\n",
    "file_list = []\n",
    "for i in range(k):\n",
    "    file_list.append('file_{}.csv'.format(i))\n",
    "# greedy_multi_splitting(complete_df, k, directory=directory, file_list=file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data from splitting folds to form training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset/fixed_dataset/fold_5/file_0.csv', '../dataset/fixed_dataset/fold_5/file_1.csv', '../dataset/fixed_dataset/fold_5/file_2.csv']\n",
      "../dataset/fixed_dataset/fold_5/file_3.csv\n"
     ]
    }
   ],
   "source": [
    "dtype_list = {'Molecule': np.str,\n",
    "              'SMILES':np.str,\n",
    "              'Fingerprints': np.str,\n",
    "              'Keck_Pria_AS_Retest': np.int64,\n",
    "              'Keck_Pria_FP_data': np.int64,\n",
    "              'Keck_Pria_Continuous': np.float64,\n",
    "              'Keck_RMI_cdd': np.float64}\n",
    "output_file_list = [directory + f_ for f_ in file_list]\n",
    "print output_file_list[:3]\n",
    "train_pd = read_merged_data(output_file_list[:3])\n",
    "print output_file_list[3]\n",
    "test_pd = read_merged_data([output_file_list[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is training set\n",
      "retest: 0, fp: 0, rmi: 0 \t--- 29693\n",
      "retest: 0, fp: 0, rmi: nan \t--- 13563\n",
      "retest: 1, fp: 0, rmi: nan \t--- 12\n",
      "retest: 0, fp: 1, rmi: 1 \t--- 1\n",
      "retest: 1, fp: 0, rmi: 0 \t--- 35\n",
      "retest: 0, fp: 0, rmi: 1 \t--- 136\n",
      "retest: 0, fp: 1, rmi: 0 \t--- 12\n",
      "\n",
      "This is test set\n",
      "retest: 0, fp: 0, rmi: 0 \t--- 9898\n",
      "retest: 0, fp: 0, rmi: nan \t--- 4521\n",
      "retest: 1, fp: 0, rmi: nan \t--- 4\n",
      "retest: 0, fp: 1, rmi: 1 \t--- 1\n",
      "retest: 1, fp: 0, rmi: 0 \t--- 11\n",
      "retest: 0, fp: 0, rmi: 1 \t--- 46\n",
      "retest: 0, fp: 1, rmi: 0 \t--- 4\n",
      "retest: 1, fp: 1, rmi: nan \t--- 1\n"
     ]
    }
   ],
   "source": [
    "print 'This is training set'\n",
    "analysis(train_pd)\n",
    "print\n",
    "print 'This is test set'\n",
    "analysis(test_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test feature- and label- extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecule                   object\n",
      "SMILES                     object\n",
      "Fingerprints               object\n",
      "Keck_Pria_AS_Retest         int64\n",
      "Keck_Pria_FP_data           int64\n",
      "Keck_Pria_Continuous      float64\n",
      "Keck_RMI_cdd              float64\n",
      "FP counts % inhibition    float64\n",
      "dtype: object\n",
      "(43452, 2)\n",
      "(14486, 2)\n"
     ]
    }
   ],
   "source": [
    "print train_pd.dtypes\n",
    "\n",
    "X_train, y_train = extract_feature_and_label(train_pd,\n",
    "                                             feature_name='Fingerprints',\n",
    "                                             label_name_list=['Keck_Pria_AS_Retest', 'Keck_Pria_FP_data'])\n",
    "X_test, y_test = extract_feature_and_label(test_pd,\n",
    "                                           feature_name='Fingerprints',\n",
    "                                           label_name_list=['Keck_Pria_AS_Retest', 'Keck_Pria_FP_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Single Classification with Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, CuDNN 4004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43452, 1)\n",
      "(14486, 1)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "config_json_file = '../json/classification.json'\n",
    "PMTNN_weight_file = 'temp.h5'\n",
    "    \n",
    "X_train, y_train = extract_feature_and_label(train_pd,\n",
    "                                             feature_name='Fingerprints',\n",
    "                                             label_name_list=['Keck_Pria_AS_Retest'])\n",
    "X_test, y_test = extract_feature_and_label(test_pd,\n",
    "                                           feature_name='Fingerprints',\n",
    "                                           label_name_list=['Keck_Pria_AS_Retest'])\n",
    "\n",
    "cross_validation_split = StratifiedShuffleSplit(y_train, 1, test_size=0.15, random_state=1)\n",
    "for t_index, val_index in cross_validation_split:\n",
    "    X_t, X_val = X_train[t_index], X_train[val_index]\n",
    "    y_t, y_val = y_train[t_index], y_train[val_index]\n",
    "\n",
    "print 'done data preparation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36934, 1024)\n",
      "(36934, 1)\n",
      "(6518, 1024)\n",
      "(6518, 1)\n",
      "(14486, 1024)\n",
      "(14486, 1)\n"
     ]
    }
   ],
   "source": [
    "print X_t.shape\n",
    "print y_t.shape\n",
    "print X_val.shape\n",
    "print y_val.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update best model\n",
      "Epoch 1/200\n",
      "Precision Train: 0.002760 ---- Precision Val: 0.001305\n",
      "ROC Train: 0.712969 ---- ROC Val: 0.596364\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.00101741557021\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 2/200\n",
      "Precision Train: 0.005747 ---- Precision Val: 0.001017\n",
      "ROC Train: 0.767141 ---- ROC Val: 0.498431\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.000983109651566\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 3/200\n",
      "Precision Train: 0.029141 ---- Precision Val: 0.000983\n",
      "ROC Train: 0.803321 ---- ROC Val: 0.483511\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.00110245443117\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 4/200\n",
      "Precision Train: 0.074346 ---- Precision Val: 0.001102\n",
      "ROC Train: 0.879710 ---- ROC Val: 0.540887\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.00102299847546\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 5/200\n",
      "Precision Train: 0.125846 ---- Precision Val: 0.001023\n",
      "ROC Train: 0.935082 ---- ROC Val: 0.516796\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.00128369090132\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 6/200\n",
      "Precision Train: 0.190796 ---- Precision Val: 0.001284\n",
      "ROC Train: 0.956178 ---- ROC Val: 0.593018\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.00124835607662\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 7/200\n",
      "Precision Train: 0.320218 ---- Precision Val: 0.001248\n",
      "ROC Train: 0.972266 ---- ROC Val: 0.566251\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.00112777024326\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 8/200\n",
      "Precision Train: 0.365757 ---- Precision Val: 0.001128\n",
      "ROC Train: 0.973786 ---- ROC Val: 0.560853\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.000761450654631\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 9/200\n",
      "Precision Train: 0.458562 ---- Precision Val: 0.000761\n",
      "ROC Train: 0.982223 ---- ROC Val: 0.365579\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "current: 0.00124750492235\tbest: 0.00130467940609\tsmaller: True\n",
      "Epoch 10/200\n",
      "Precision Train: 0.495703 ---- Precision Val: 0.001248\n",
      "ROC Train: 0.992771 ---- ROC Val: 0.487022\n",
      "Best ROC is: 0.00130467940609\n",
      "\n",
      "update best model\n",
      "Epoch 11/200\n",
      "Precision Train: 0.605352 ---- Precision Val: 0.002405\n",
      "ROC Train: 0.992882 ---- ROC Val: 0.630472\n",
      "Best ROC is: 0.0024051858891\n",
      "\n",
      "update best model\n",
      "Epoch 12/200\n",
      "Precision Train: 0.695238 ---- Precision Val: 0.003729\n",
      "ROC Train: 0.993512 ---- ROC Val: 0.707155\n",
      "Best ROC is: 0.00372861684082\n",
      "\n",
      "current: 0.00283463740815\tbest: 0.00372861684082\tsmaller: True\n",
      "Epoch 13/200\n",
      "Precision Train: 0.710384 ---- Precision Val: 0.002835\n",
      "ROC Train: 0.995853 ---- ROC Val: 0.678720\n",
      "Best ROC is: 0.00372861684082\n",
      "\n",
      "current: 0.00229733493937\tbest: 0.00372861684082\tsmaller: True\n",
      "Epoch 14/200\n",
      "Precision Train: 0.833960 ---- Precision Val: 0.002297\n",
      "ROC Train: 0.997645 ---- ROC Val: 0.616100\n",
      "Best ROC is: 0.00372861684082\n",
      "\n",
      "update best model\n",
      "Epoch 15/200\n",
      "Precision Train: 0.863434 ---- Precision Val: 0.004021\n",
      "ROC Train: 0.997916 ---- ROC Val: 0.662110\n",
      "Best ROC is: 0.00402143124186\n",
      "\n",
      "current: 0.00276453762466\tbest: 0.00402143124186\tsmaller: True\n",
      "Epoch 16/200\n",
      "Precision Train: 0.848409 ---- Precision Val: 0.002765\n",
      "ROC Train: 0.995991 ---- ROC Val: 0.727933\n",
      "Best ROC is: 0.00402143124186\n",
      "\n",
      "current: 0.00366266301518\tbest: 0.00402143124186\tsmaller: True\n",
      "Epoch 17/200\n",
      "Precision Train: 0.905209 ---- Precision Val: 0.003663\n",
      "ROC Train: 0.999318 ---- ROC Val: 0.631920\n",
      "Best ROC is: 0.00402143124186\n",
      "\n",
      "update best model\n",
      "Epoch 18/200\n",
      "Precision Train: 0.876266 ---- Precision Val: 0.005108\n",
      "ROC Train: 0.999759 ---- ROC Val: 0.709656\n",
      "Best ROC is: 0.00510831055899\n",
      "\n",
      "current: 0.00386441468468\tbest: 0.00510831055899\tsmaller: True\n",
      "Epoch 19/200\n",
      "Precision Train: 0.941274 ---- Precision Val: 0.003864\n",
      "ROC Train: 0.999837 ---- ROC Val: 0.700353\n",
      "Best ROC is: 0.00510831055899\n",
      "\n",
      "current: 0.00221049797614\tbest: 0.00510831055899\tsmaller: True\n",
      "Epoch 20/200\n",
      "Precision Train: 0.929611 ---- Precision Val: 0.002210\n",
      "ROC Train: 0.999912 ---- ROC Val: 0.614169\n",
      "Best ROC is: 0.00510831055899\n",
      "\n",
      "update best model\n",
      "Epoch 21/200\n",
      "Precision Train: 0.965751 ---- Precision Val: 0.014968\n",
      "ROC Train: 0.999853 ---- ROC Val: 0.687386\n",
      "Best ROC is: 0.0149683655035\n",
      "\n",
      "current: 0.00527646986261\tbest: 0.0149683655035\tsmaller: True\n",
      "Epoch 22/200\n",
      "Precision Train: 0.969144 ---- Precision Val: 0.005276\n",
      "ROC Train: 0.999928 ---- ROC Val: 0.639116\n",
      "Best ROC is: 0.0149683655035\n",
      "\n",
      "current: 0.0113227736131\tbest: 0.0149683655035\tsmaller: True\n",
      "Epoch 23/200\n",
      "Precision Train: 0.992097 ---- Precision Val: 0.011323\n",
      "ROC Train: 0.999990 ---- ROC Val: 0.657349\n",
      "Best ROC is: 0.0149683655035\n",
      "\n",
      "current: 0.00503376539708\tbest: 0.0149683655035\tsmaller: True\n",
      "Epoch 24/200\n",
      "Precision Train: 0.967957 ---- Precision Val: 0.005034\n",
      "ROC Train: 0.999899 ---- ROC Val: 0.686728\n",
      "Best ROC is: 0.0149683655035\n",
      "\n",
      "current: 0.0138050756363\tbest: 0.0149683655035\tsmaller: True\n",
      "Epoch 25/200\n",
      "Precision Train: 0.981444 ---- Precision Val: 0.013805\n",
      "ROC Train: 0.999978 ---- ROC Val: 0.672817\n",
      "Best ROC is: 0.0149683655035\n",
      "\n",
      "current: 0.0127779606982\tbest: 0.0149683655035\tsmaller: True\n",
      "Epoch 26/200\n",
      "Precision Train: 0.985443 ---- Precision Val: 0.012778\n",
      "ROC Train: 0.999983 ---- ROC Val: 0.744411\n",
      "Best ROC is: 0.0149683655035\n",
      "\n",
      "current: 0.0130823585301\tbest: 0.0149683655035\tsmaller: True\n",
      "Epoch 27/200\n",
      "Precision Train: 0.993374 ---- Precision Val: 0.013082\n",
      "ROC Train: 0.999993 ---- ROC Val: 0.717028\n",
      "Best ROC is: 0.0149683655035\n",
      "\n",
      "update best model\n",
      "Epoch 28/200\n",
      "Precision Train: 0.998235 ---- Precision Val: 0.046944\n",
      "ROC Train: 0.999998 ---- ROC Val: 0.696163\n",
      "Best ROC is: 0.0469438158743\n",
      "\n",
      "current: 0.013649199029\tbest: 0.0469438158743\tsmaller: True\n",
      "Epoch 29/200\n",
      "Precision Train: 0.999383 ---- Precision Val: 0.013649\n",
      "ROC Train: 0.999999 ---- ROC Val: 0.684380\n",
      "Best ROC is: 0.0469438158743\n",
      "\n",
      "update best model\n",
      "Epoch 30/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.052892\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.716677\n",
      "Best ROC is: 0.0528923703309\n",
      "\n",
      "current: 0.00917557600075\tbest: 0.0528923703309\tsmaller: True\n",
      "Epoch 31/200\n",
      "Precision Train: 0.998100 ---- Precision Val: 0.009176\n",
      "ROC Train: 0.999998 ---- ROC Val: 0.665226\n",
      "Best ROC is: 0.0528923703309\n",
      "\n",
      "current: 0.039619725862\tbest: 0.0528923703309\tsmaller: True\n",
      "Epoch 32/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.039620\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.754613\n",
      "Best ROC is: 0.0528923703309\n",
      "\n",
      "update best model\n",
      "Epoch 33/200\n",
      "Precision Train: 0.999383 ---- Precision Val: 0.059330\n",
      "ROC Train: 0.999999 ---- ROC Val: 0.779231\n",
      "Best ROC is: 0.0593295317131\n",
      "\n",
      "update best model\n",
      "Epoch 34/200\n",
      "Precision Train: 0.998235 ---- Precision Val: 0.085184\n",
      "ROC Train: 0.999998 ---- ROC Val: 0.836749\n",
      "Best ROC is: 0.0851844652482\n",
      "\n",
      "current: 0.0354876601359\tbest: 0.0851844652482\tsmaller: True\n",
      "Epoch 35/200\n",
      "Precision Train: 0.999383 ---- Precision Val: 0.035488\n",
      "ROC Train: 0.999999 ---- ROC Val: 0.836189\n",
      "Best ROC is: 0.0851844652482\n",
      "\n",
      "current: 0.0337240254765\tbest: 0.0851844652482\tsmaller: True\n",
      "Epoch 36/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.033724\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.806394\n",
      "Best ROC is: 0.0851844652482\n",
      "\n",
      "current: 0.0581996220195\tbest: 0.0851844652482\tsmaller: True\n",
      "Epoch 37/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.058200\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.903043\n",
      "Best ROC is: 0.0851844652482\n",
      "\n",
      "current: 0.0698637910962\tbest: 0.0851844652482\tsmaller: True\n",
      "Epoch 38/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.069864\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.866051\n",
      "Best ROC is: 0.0851844652482\n",
      "\n",
      "current: 0.0682145682764\tbest: 0.0851844652482\tsmaller: True\n",
      "Epoch 39/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.068215\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.863242\n",
      "Best ROC is: 0.0851844652482\n",
      "\n",
      "update best model\n",
      "Epoch 40/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.106391\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.864537\n",
      "Best ROC is: 0.106390516886\n",
      "\n",
      "update best model\n",
      "Epoch 41/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.142058\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.826908\n",
      "Best ROC is: 0.142058281774\n",
      "\n",
      "update best model\n",
      "Epoch 42/200\n",
      "Precision Train: 0.989555 ---- Precision Val: 0.197562\n",
      "ROC Train: 0.999991 ---- ROC Val: 0.837330\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.153008898407\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 43/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.153009\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.785594\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.167499284368\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 44/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.167499\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.800338\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.104426161093\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 45/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.104426\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.808390\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.129024360149\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 46/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.129024\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.785528\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.128984208606\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 47/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.128984\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.769138\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.100783195559\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 48/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.100783\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.780438\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.130437792978\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 49/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.130438\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.813064\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.140864226321\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 50/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.140864\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.794962\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.159225274153\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 51/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.159225\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.817518\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.163332492173\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 52/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.163332\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.810848\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.196615838872\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 53/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.196616\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.810804\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "current: 0.182385109646\tbest: 0.197561697659\tsmaller: True\n",
      "Epoch 54/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.182385\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.777629\n",
      "Best ROC is: 0.197561697659\n",
      "\n",
      "update best model\n",
      "Epoch 55/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.208080\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.827259\n",
      "Best ROC is: 0.20808008692\n",
      "\n",
      "update best model\n",
      "Epoch 56/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.208364\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.781205\n",
      "Best ROC is: 0.208363563468\n",
      "\n",
      "current: 0.207778100858\tbest: 0.208363563468\tsmaller: True\n",
      "Epoch 57/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.207778\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.773175\n",
      "Best ROC is: 0.208363563468\n",
      "\n",
      "update best model\n",
      "Epoch 58/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.209013\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.753209\n",
      "Best ROC is: 0.209012807619\n",
      "\n",
      "current: 0.207434113285\tbest: 0.209012807619\tsmaller: True\n",
      "Epoch 59/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.207434\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.783729\n",
      "Best ROC is: 0.209012807619\n",
      "\n",
      "current: 0.181910105277\tbest: 0.209012807619\tsmaller: True\n",
      "Epoch 60/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.181910\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.750422\n",
      "Best ROC is: 0.209012807619\n",
      "\n",
      "current: 0.178142700559\tbest: 0.209012807619\tsmaller: True\n",
      "Epoch 61/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.178143\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.776598\n",
      "Best ROC is: 0.209012807619\n",
      "\n",
      "current: 0.147612042085\tbest: 0.209012807619\tsmaller: True\n",
      "Epoch 62/200\n",
      "Precision Train: 0.968683 ---- Precision Val: 0.147612\n",
      "ROC Train: 0.999980 ---- ROC Val: 0.754262\n",
      "Best ROC is: 0.209012807619\n",
      "\n",
      "current: 0.150746166841\tbest: 0.209012807619\tsmaller: True\n",
      "Epoch 63/200\n",
      "Precision Train: 0.998100 ---- Precision Val: 0.150746\n",
      "ROC Train: 0.999998 ---- ROC Val: 0.715558\n",
      "Best ROC is: 0.209012807619\n",
      "\n",
      "current: 0.207900970897\tbest: 0.209012807619\tsmaller: True\n",
      "Epoch 64/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.207901\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.745903\n",
      "Best ROC is: 0.209012807619\n",
      "\n",
      "update best model\n",
      "Epoch 65/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.212170\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.785045\n",
      "Best ROC is: 0.212169722153\n",
      "\n",
      "current: 0.186135147557\tbest: 0.212169722153\tsmaller: True\n",
      "Epoch 66/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.186135\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.757465\n",
      "Best ROC is: 0.212169722153\n",
      "\n",
      "update best model\n",
      "Epoch 67/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.224672\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.771376\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.223633752229\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 68/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.223634\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.743248\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.224389579438\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 69/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.224390\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.815850\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.196283965978\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 70/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.196284\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.816113\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.189300110827\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 71/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.189300\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.845426\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.194640709074\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 72/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.194641\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.836650\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.194270770076\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 73/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.194271\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.822191\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.212895817853\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 74/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.212896\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.806920\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.182102223137\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 75/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.182102\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.767624\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.181887428904\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 76/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.181887\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.780438\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.175617627021\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 77/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.175618\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.817540\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.196073720652\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 78/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.196074\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.797902\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.196051305461\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 79/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.196051\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.776335\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.182297646967\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 80/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.182298\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.786318\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.196591527479\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 81/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.196592\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.811374\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.189025239496\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 82/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.189025\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.805165\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.173690855084\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 83/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.173691\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.875398\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.194364380096\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 84/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.194364\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.841389\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.194396272197\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 85/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.194396\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.823749\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.182428269484\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 86/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.182428\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.830002\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.188265649888\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 87/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.188266\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.796279\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.219380386265\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 88/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.219380\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.799855\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.220421647442\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 89/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.220422\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.810255\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.203270565456\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 90/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.203271\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.849222\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.216229252648\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 91/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.216229\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.837242\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.215470783371\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 92/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.215471\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.813283\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.215526053514\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 93/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.215526\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.800064\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.208047817144\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 94/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.208048\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.824999\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.207491328166\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 95/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.207491\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.811067\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.207564060516\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 96/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.207564\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.831362\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.207359063157\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 97/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.207359\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.817869\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.196174181866\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 98/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.196174\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.814358\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.194818908031\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 99/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.194819\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.844790\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.194988087923\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 100/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.194988\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.843912\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.184043549571\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 101/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.184044\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.854334\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.197291287766\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 102/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.197291\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.864647\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.202604184564\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 103/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.202604\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.857560\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.208461663588\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 104/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.208462\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.852557\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.209480530526\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 105/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.209481\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.859732\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.202182293998\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 106/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.202182\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.838581\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.19685494079\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 107/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.196855\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.838361\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.198115173585\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 108/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.198115\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.846370\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.209011007845\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 109/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.209011\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.855497\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.175819728484\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 110/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.175820\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.850824\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.179689205972\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 111/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.179689\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.838844\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.193002932758\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 112/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.193003\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.847752\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.180044476884\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 113/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.180044\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.851153\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.193925602052\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 114/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.193926\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.867784\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.199457886394\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 115/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.199458\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.864625\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.194861212697\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 116/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.194861\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.829388\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.197736479195\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 117/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.197736\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.813612\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "current: 0.189012418132\tbest: 0.224672136809\tsmaller: True\n",
      "Epoch 118/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.189012\n",
      "ROC Train: 1.000000 ---- ROC Val: 0.845723\n",
      "Best ROC is: 0.224672136809\n",
      "\n",
      "get best auc or precision\n",
      "get best model\n",
      "train precision: 1.0\n",
      "train roc: 1.0\n",
      "validation precision: 0.189012418132\n",
      "validation roc: 0.845722623253\n",
      "test precision: 0.052379568933\n",
      "test roc: 0.820961472011\n",
      "ratio: 0.02, EF: 18.75,\tactive: 16.0\n",
      "ratio: 0.01, EF: 37.5,\tactive: 16.0\n",
      "ratio: 0.0015, EF: 166.666666667,\tactive: 16.0\n",
      "ratio: 0.001, EF: 62.5,\tactive: 16.0\n"
     ]
    }
   ],
   "source": [
    "from single_task import *\n",
    "\n",
    "\n",
    "class single_task:\n",
    "    def __init__(self, config_json_file):\n",
    "        with open(config_json_file, 'r') as f:\n",
    "            conf = json.load(f)\n",
    "\n",
    "        self.conf = conf\n",
    "        self.input_layer_dimension = 1024\n",
    "        self.output_layer_dimension = 1\n",
    "\n",
    "        self.early_stopping_patience = conf['fitting']['early_stopping']['patience']\n",
    "        self.early_stopping_option = conf['fitting']['early_stopping']['option']\n",
    "\n",
    "        self.fit_nb_epoch = conf['fitting']['nb_epoch']\n",
    "        self.fit_batch_size = conf['fitting']['batch_size']\n",
    "        self.fit_verbose = conf['fitting']['verbose']\n",
    "\n",
    "        self.compile_loss = conf['compile']['loss']\n",
    "        self.compile_optimizer_option = conf['compile']['optimizer']['option']\n",
    "        if self.compile_optimizer_option == 'sgd':\n",
    "            sgd_lr = conf['compile']['optimizer']['sgd']['lr']\n",
    "            sgd_momentum = conf['compile']['optimizer']['sgd']['momentum']\n",
    "            sgd_decay = conf['compile']['optimizer']['sgd']['decay']\n",
    "            sgd_nestrov = conf['compile']['optimizer']['sgd']['nestrov']\n",
    "            self.compile_optimizer = SGD(lr=sgd_lr, momentum=sgd_momentum, decay=sgd_decay, nesterov=sgd_nestrov)\n",
    "        else:\n",
    "            adam_lr = conf['compile']['optimizer']['adam']['lr']\n",
    "            adam_beta_1 = conf['compile']['optimizer']['adam']['beta_1']\n",
    "            adam_beta_2 = conf['compile']['optimizer']['adam']['beta_2']\n",
    "            adam_epsilon = conf['compile']['optimizer']['adam']['epsilon']\n",
    "            self.compile_optimizer = Adam(lr=adam_lr, beta_1=adam_beta_1, beta_2=adam_beta_2, epsilon=adam_epsilon)\n",
    "\n",
    "        self.batch_is_use = conf['batch']['is_use']\n",
    "        if self.batch_is_use:\n",
    "            batch_normalizer_epsilon = conf['batch']['epsilon']\n",
    "            batch_normalizer_mode = conf['batch']['mode']\n",
    "            batch_normalizer_axis = conf['batch']['axis']\n",
    "            batch_normalizer_momentum = conf['batch']['momentum']\n",
    "            batch_normalizer_weights = conf['batch']['weights']\n",
    "            batch_normalizer_beta_init = conf['batch']['beta_init']\n",
    "            batch_normalizer_gamma_init = conf['batch']['gamma_init']\n",
    "            self.batch_normalizer = BatchNormalization(epsilon=batch_normalizer_epsilon,\n",
    "                                                       mode=batch_normalizer_mode,\n",
    "                                                       axis=batch_normalizer_axis,\n",
    "                                                       momentum=batch_normalizer_momentum,\n",
    "                                                       weights=batch_normalizer_weights,\n",
    "                                                       beta_init=batch_normalizer_beta_init,\n",
    "                                                       gamma_init=batch_normalizer_gamma_init)\n",
    "        self.EF_ratio_list = conf['enrichment_factor']['ratio_list']\n",
    "\n",
    "        return\n",
    "\n",
    "    def setup_model(self):\n",
    "        model = Sequential()\n",
    "        if self.batch_is_use:\n",
    "            batch_normalizer = self.batch_normalizer\n",
    "        layers = self.conf['layers']\n",
    "        layer_number = len(layers)\n",
    "        for i in range(layer_number):\n",
    "            init = layers[i]['init']\n",
    "            activation = layers[i]['activation']\n",
    "            if i == 0:\n",
    "                hidden_units = int(layers[i]['hidden_units'])\n",
    "                dropout = float(layers[i]['dropout'])\n",
    "                model.add(Dense(hidden_units, input_dim=self.input_layer_dimension, init=init, activation=activation))\n",
    "                model.add(Dropout(dropout))\n",
    "            elif i == layer_number - 1:\n",
    "                if self.batch_is_use:\n",
    "                    model.add(self.batch_normalizer)\n",
    "                model.add(Dense(self.output_layer_dimension, init=init, activation=activation))\n",
    "            else:\n",
    "                hidden_units = int(layers[i]['hidden_units'])\n",
    "                dropout = float(layers[i]['dropout'])\n",
    "                model.add(Dense(hidden_units, init=init, activation=activation))\n",
    "                model.add(Dropout(dropout))\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train_and_predict(self,\n",
    "                          X_train, y_train,\n",
    "                          X_val, y_val,\n",
    "                          X_test, y_test,\n",
    "                          PMTNN_weight_file):\n",
    "        model = self.setup_model()\n",
    "        if self.early_stopping_option == 'auc':\n",
    "            early_stopping = KeckCallBackOnAUC(X_train, y_train, X_val, y_val, patience=self.early_stopping_patience)\n",
    "            callbacks = [early_stopping]\n",
    "        elif self.early_stopping_option == 'precision':\n",
    "            early_stopping = KeckCallBackOnPrecision(X_train, y_train, X_val, y_val,\n",
    "                                                     patience=self.early_stopping_patience)\n",
    "            callbacks = [early_stopping]\n",
    "        else:\n",
    "            callbacks = []\n",
    "\n",
    "        model.compile(loss=self.compile_loss, optimizer=self.compile_optimizer)\n",
    "        model.fit(X_train, y_train,\n",
    "                  nb_epoch=self.fit_nb_epoch,\n",
    "                  batch_size=self.fit_batch_size,\n",
    "                  verbose=self.fit_verbose,\n",
    "                  validation_data=(X_val, y_val),\n",
    "                  callbacks=callbacks)\n",
    "        model.save_weights(PMTNN_weight_file)\n",
    "\n",
    "        if self.early_stopping_option == 'auc' or self.early_stopping_option == 'precision':\n",
    "            print 'get best auc or precision'\n",
    "            model = early_stopping.get_best_model()\n",
    "        y_pred_on_train = model.predict(X_train)\n",
    "        y_pred_on_val = model.predict(X_val)\n",
    "        y_pred_on_test = model.predict(X_test)\n",
    "\n",
    "        print('train precision: {}'.format(average_precision_score(y_train, y_pred_on_train)))\n",
    "        print('train roc: {}'.format(roc_auc_score(y_train, y_pred_on_train)))\n",
    "        print('validation precision: {}'.format(average_precision_score(y_val, y_pred_on_val)))\n",
    "        print('validation roc: {}'.format(roc_auc_score(y_val, y_pred_on_val)))\n",
    "        print('test precision: {}'.format(average_precision_score(y_test, y_pred_on_test)))\n",
    "        print('test roc: {}'.format(roc_auc_score(y_test, y_pred_on_test)))\n",
    "\n",
    "        for EF_ratio in self.EF_ratio_list:\n",
    "            n_actives, ef = enrichment_factor(y_test, y_pred_on_test, EF_ratio)\n",
    "            print('ratio: {}, EF: {},\\tactive: {}'.format(EF_ratio, ef, n_actives))\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict_with_existing(self,\n",
    "                              X_train, y_train,\n",
    "                              X_val, y_val,\n",
    "                              X_test, y_test,\n",
    "                              PMTNN_weight_file):\n",
    "        model = setup_model()\n",
    "        model.load_weights(PMTNN_weight_file)\n",
    "\n",
    "        y_pred_on_train = model.predict(X_train)\n",
    "        y_pred_on_val = model.predict(X_val)\n",
    "        y_pred_on_test = model.predict(X_test)\n",
    "\n",
    "        print('train precision: {}'.format(average_precision_score(y_train, y_pred_on_train)))\n",
    "        print('train auc: {}'.format(roc_auc_score(y_train, y_pred_on_train)))\n",
    "        print('validation precision: {}'.format(average_precision_score(y_val, y_pred_on_val)))\n",
    "        print('validation auc: {}'.format(roc_auc_score(y_val, y_pred_on_val)))\n",
    "        print('test precision: {}'.format(average_precision_score(y_test, y_pred_on_test)))\n",
    "        print('test auc: {}'.format(roc_auc_score(y_test, y_pred_on_test)))\n",
    "\n",
    "        return\n",
    "\n",
    "    def get_EF_score_with_existing_model(self,\n",
    "                                         X_test, y_test,\n",
    "                                         file_path, EF_ratio):\n",
    "        model = setup_model()\n",
    "        model.load_weights(file_path)\n",
    "        y_pred_on_test = model.predict(X_test)\n",
    "        n_actives, ef = enrichment_factor(y_test, y_pred_on_test, EF_ratio)\n",
    "        print('test precision: {}'.format(average_precision_score(y_test, y_pred_on_test)))\n",
    "        print('test auc: {}'.format(roc_auc_score(y_test, y_pred_on_test)))\n",
    "        print('EF: {},\\tactive: {}'.format(ef, n_actives))\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "def enrichment_factor(labels_arr, scores_arr, percentile):\n",
    "    '''calculate the enrichment factor based on some upper fraction\n",
    "       of library ordered by docking scores. upper fraction is determined\n",
    "       by percentile (actually a fraction of value 0.0-1.0)'''\n",
    "    sample_size = int(labels_arr.shape[0] * percentile)  # determine number mols in subset\n",
    "    pred = np.sort(scores_arr)[::-1][:sample_size]  # sort the scores list, take top subset from library\n",
    "    indices = np.argsort(scores_arr, axis=0)[::-1][:sample_size]  # get the index positions for these in library\n",
    "    n_actives = np.nansum(labels_arr)  # count number of positive labels in library\n",
    "    n_experimental = np.nansum(labels_arr[indices])  # count number of positive labels in subset\n",
    "    if n_actives > 0.0:\n",
    "        ef = float(n_experimental) / n_actives / percentile  # calc EF at percentile\n",
    "    else:\n",
    "        ef = 'ND'\n",
    "    return n_actives, ef\n",
    "\n",
    "# define custom classes\n",
    "# following class is used for keras to compute the AUC each epoch\n",
    "# and do early stoppping based on that\n",
    "class KeckCallBackOnAUC(keras.callbacks.Callback):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, patience=0):\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        self.curr_auc = 0\n",
    "        self.best_auc = 0\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.best_model = None\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.nb_epoch = self.params['nb_epoch']\n",
    "        self.curr_auc = roc_auc_score(self.y_val, self.model.predict(self.X_val))\n",
    "        self.best_auc = self.curr_auc\n",
    "        self.best_model = self.model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.curr_auc = roc_auc_score(self.y_val, self.model.predict(self.X_val))\n",
    "        if self.curr_auc < self.best_auc:\n",
    "            print 'current: {}\\tbest: {}\\tsmaller: {}'.format(self.curr_auc,self.best_auc,self.curr_auc<self.best_auc)\n",
    "            if self.counter >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:\n",
    "            print 'update best model'\n",
    "            print 'current: {}\\tbest: {}\\tsmaller: {}'.format(self.curr_auc,self.best_auc,self.curr_auc<self.best_auc)\n",
    "            self.counter = 0\n",
    "            self.best_auc = self.curr_auc\n",
    "            self.best_model = self.model\n",
    "        train_auc = roc_auc_score(self.y_train, self.model.predict(self.X_train))\n",
    "        train_precision = average_precision_score(self.y_train, self.model.predict(self.X_train))\n",
    "        curr_precision = average_precision_score(self.y_val, self.model.predict(self.X_val))\n",
    "        print('Epoch %d/%d' % (epoch + 1, self.nb_epoch))\n",
    "        print('ROC Train: %f ---- ROC Val: %f' % (train_auc, self.curr_auc))\n",
    "        print('Precision Train: %f ---- Precision Val: %f' % (train_precision, curr_precision))\n",
    "        print\n",
    "\n",
    "    def get_best_model(self):\n",
    "        return self.best_model\n",
    "\n",
    "    def get_best_auc(self):\n",
    "        return self.best_auc\n",
    "\n",
    "\n",
    "# define custom classes\n",
    "# following class is used for keras to compute the precision each epoch\n",
    "# and do early stoppping based on that\n",
    "class KeckCallBackOnPrecision(keras.callbacks.Callback):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val, patience=0):\n",
    "        super(keras.callbacks.Callback, self).__init__()\n",
    "        self.curr_precision = 0\n",
    "        self.best_precision = 0\n",
    "        self.counter = 0\n",
    "        self.patience = patience\n",
    "        self.best_model = None\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.nb_epoch = self.params['nb_epoch']\n",
    "        self.curr_precision = average_precision_score(self.y_val, self.model.predict(self.X_val))\n",
    "        self.best_precision = self.curr_precision\n",
    "        self.best_model = self.model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.curr_precision = average_precision_score(self.y_val, self.model.predict(self.X_val))\n",
    "        if self.curr_precision < self.best_precision:\n",
    "            print 'current: {}\\tbest: {}\\tsmaller: {}'.format(self.curr_precision,self.best_precision,self.curr_precision<self.best_precision)\n",
    "            if self.counter >= self.patience:\n",
    "                self.model.stop_training = True\n",
    "            else:\n",
    "                self.counter += 1\n",
    "        else:\n",
    "            print 'update best model'\n",
    "            self.counter = 0\n",
    "            self.best_precision = self.curr_precision\n",
    "            self.best_model = copy.deepcopy(self.model)\n",
    "            \n",
    "        train_precision = average_precision_score(self.y_train, self.model.predict(self.X_train))\n",
    "        train_auc = roc_auc_score(self.y_train, self.model.predict(self.X_train))\n",
    "        curr_auc = roc_auc_score(self.y_val, self.model.predict(self.X_val))\n",
    "        print('Epoch %d/%d' % (epoch + 1, self.nb_epoch))\n",
    "        print('Precision Train: %f ---- Precision Val: %f' % (train_precision, self.curr_precision))\n",
    "        print('ROC Train: %f ---- ROC Val: %f' % (train_auc, curr_auc))\n",
    "        print 'Best ROC is: {}'.format(self.best_precision)\n",
    "        print\n",
    "\n",
    "    def get_best_model(self):\n",
    "        print 'get best model'\n",
    "        return self.best_model\n",
    "\n",
    "    def get_best_precision(self):\n",
    "        return self.best_precision\n",
    "\n",
    "PMTNN_weight_file = 'temp.h5'\n",
    "\n",
    "task = single_task(config_json_file=config_json_file)\n",
    "task.EF_ratio_list = conf['enrichment_factor']['ratio_list']\n",
    "task.train_and_predict(X_t, y_t, X_val, y_val, X_test, y_test, PMTNN_weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[1 2 3]\n",
      "[ 1 10  3]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = copy.deepcopy(a)\n",
    "print a\n",
    "print b\n",
    "\n",
    "\n",
    "\n",
    "a[1] = 10\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot EF and EF-Max graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (25.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbIAAAJoCAYAAACz5xdKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc3a1210f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_EF_score_with_existing_model(X_test, y_test, file_path, EF_ratio=0.1):\n",
    "    model = task.setup_model()\n",
    "    model.load_weights(file_path)\n",
    "    y_pred_on_test = model.predict(X_test)\n",
    "    EF_ratio_list = [0.1, 0.5, 1, 2, 3, 4, 5, 10, 15, 20]\n",
    "    EF_ratio_list = np.array(EF_ratio_list) / 100.0\n",
    "    \n",
    "    ef_values = []\n",
    "    ef_max_values = []\n",
    "    for EF_ratio in EF_ratio_list:\n",
    "        n_actives, ef, ef_max = enrichment_factor_single(y_test, y_pred_on_test, EF_ratio)\n",
    "        ef_values.append(ef)\n",
    "        ef_max_values.append(ef_max)\n",
    "    \n",
    "    x_axis = EF_ratio_list\n",
    "    y_axis = np.array(ef_values)\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    \n",
    "    x_axis = EF_ratio_list\n",
    "    y_axis = np.array(ef_max_values)\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    \n",
    "    plt.legend(['EF', 'EF Max'])\n",
    "    \n",
    "    plt.xticks(EF_ratio_list, [str(val*100) for val in EF_ratio_list])\n",
    "    plt.xlabel('Enrichment Factor Percentile (%)')\n",
    "    plt.ylabel('Enrichment Factor')\n",
    "    plt.title('EF Curve')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_EF_score_with_existing_model(X_test, y_test, PMTNN_weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
