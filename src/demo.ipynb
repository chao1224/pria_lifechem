{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from function import *\n",
    "from data_preparation import *\n",
    "from evaluation import *\n",
    "\n",
    "from openpyxl import Workbook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-prepare Data (Data Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform_data(output_file_name='../dataset/keck_complete.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecule                   object\n",
      "SMILES                     object\n",
      "Fingerprints               object\n",
      "Keck_Pria_AS_Retest         int64\n",
      "Keck_Pria_FP_data           int64\n",
      "Keck_Pria_Continuous      float64\n",
      "Keck_RMI_cdd              float64\n",
      "FP counts % inhibition    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "complete_df = pd.read_csv('../dataset/keck_complete.csv')\n",
    "\n",
    "print complete_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pria retest active: 79\tpria fp active: 24\trmi cdd active: 230\n",
      "{0: 72094, 1: 325, 2: 4}\n",
      "\n",
      "retest: 0, fp: 0, rmi: 0 \t--- 49489\n",
      "retest: 0, fp: 0, rmi: nan \t--- 22605\n",
      "retest: 0, fp: 1, rmi: 0 \t--- 19\n",
      "retest: 0, fp: 1, rmi: 1 \t--- 3\n",
      "retest: 1, fp: 0, rmi: 0 \t--- 58\n",
      "retest: 1, fp: 0, rmi: nan \t--- 20\n",
      "retest: 0, fp: 1, rmi: nan \t--- 1\n",
      "retest: 0, fp: 0, rmi: 1 \t--- 227\n",
      "retest: 1, fp: 1, rmi: nan \t--- 1\n"
     ]
    }
   ],
   "source": [
    "cnt_pria_retest = 0\n",
    "cnt_pria_fp = 0\n",
    "cnt_rmi_cdd = 0\n",
    "\n",
    "cnt_dict = {}\n",
    "for ix, row in complete_df.iterrows():\n",
    "    cnt = 0\n",
    "    if row['Keck_Pria_AS_Retest'] == 1:\n",
    "        cnt_pria_retest += 1\n",
    "        cnt += 1\n",
    "    if row['Keck_Pria_FP_data'] == 1:\n",
    "        cnt_pria_fp += 1\n",
    "        cnt += 1\n",
    "    if row['Keck_RMI_cdd'] == 1:\n",
    "        cnt_rmi_cdd += 1\n",
    "        cnt += 1\n",
    "    if cnt not in cnt_dict.keys():\n",
    "        cnt_dict[cnt] = 0\n",
    "    cnt_dict[cnt] += 1\n",
    "\n",
    "print 'pria retest active: {}\\tpria fp active: {}\\trmi cdd active: {}'.format(cnt_pria_retest, cnt_pria_fp, cnt_rmi_cdd)\n",
    "print cnt_dict\n",
    "\n",
    "\n",
    "print\n",
    "analysis(complete_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(24138, 8)\n",
      "(24143, 8)\n",
      "(24142, 8)\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "directory = '../dataset/fixed_dataset/fold_{}/'.format(k)\n",
    "file_list = []\n",
    "for i in range(k):\n",
    "    file_list.append('file_{}.csv'.format(i))\n",
    "greedy_multi_splitting(complete_df, k, directory=directory, file_list=file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(18105, 8)\n",
      "(18107, 8)\n",
      "(18106, 8)\n",
      "(18105, 8)\n"
     ]
    }
   ],
   "source": [
    "k = 4\n",
    "directory = '../dataset/fixed_dataset/fold_{}/'.format(k)\n",
    "file_list = []\n",
    "for i in range(k):\n",
    "    file_list.append('file_{}.csv'.format(i))\n",
    "greedy_multi_splitting(complete_df, k, directory=directory, file_list=file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(14486, 8)\n",
      "(14482, 8)\n",
      "(14484, 8)\n",
      "(14485, 8)\n",
      "(14486, 8)\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "directory = '../dataset/fixed_dataset/fold_{}/'.format(k)\n",
    "file_list = []\n",
    "for i in range(k):\n",
    "    file_list.append('file_{}.csv'.format(i))\n",
    "greedy_multi_splitting(complete_df, k, directory=directory, file_list=file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge data from splitting folds to form training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset/fixed_dataset/fold_5/file_0.csv', '../dataset/fixed_dataset/fold_5/file_1.csv', '../dataset/fixed_dataset/fold_5/file_2.csv']\n",
      "../dataset/fixed_dataset/fold_5/file_3.csv\n"
     ]
    }
   ],
   "source": [
    "dtype_list = {'Molecule': np.str,\n",
    "              'SMILES':np.str,\n",
    "              'Fingerprints': np.str,\n",
    "              'Keck_Pria_AS_Retest': np.int64,\n",
    "              'Keck_Pria_FP_data': np.int64,\n",
    "              'Keck_Pria_Continuous': np.float64,\n",
    "              'Keck_RMI_cdd': np.float64}\n",
    "output_file_list = [directory + f_ for f_ in file_list]\n",
    "print output_file_list[:3]\n",
    "train_pd = read_merged_data(output_file_list[:3])\n",
    "print output_file_list[3]\n",
    "test_pd = read_merged_data([output_file_list[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is training set\n",
      "retest: 0, fp: 0, rmi: 0 \t--- 29693\n",
      "retest: 0, fp: 0, rmi: nan \t--- 13563\n",
      "retest: 1, fp: 0, rmi: nan \t--- 12\n",
      "retest: 0, fp: 1, rmi: 1 \t--- 1\n",
      "retest: 1, fp: 0, rmi: 0 \t--- 35\n",
      "retest: 0, fp: 0, rmi: 1 \t--- 136\n",
      "retest: 0, fp: 1, rmi: 0 \t--- 12\n",
      "\n",
      "This is test set\n",
      "retest: 0, fp: 0, rmi: 0 \t--- 9898\n",
      "retest: 0, fp: 0, rmi: nan \t--- 4521\n",
      "retest: 1, fp: 0, rmi: nan \t--- 4\n",
      "retest: 0, fp: 1, rmi: 1 \t--- 1\n",
      "retest: 1, fp: 0, rmi: 0 \t--- 12\n",
      "retest: 0, fp: 1, rmi: nan \t--- 1\n",
      "retest: 0, fp: 0, rmi: 1 \t--- 45\n",
      "retest: 0, fp: 1, rmi: 0 \t--- 3\n"
     ]
    }
   ],
   "source": [
    "print 'This is training set'\n",
    "analysis(train_pd)\n",
    "print\n",
    "print 'This is test set'\n",
    "analysis(test_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test feature- and label- extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecule                   object\n",
      "SMILES                     object\n",
      "Fingerprints               object\n",
      "Keck_Pria_AS_Retest         int64\n",
      "Keck_Pria_FP_data           int64\n",
      "Keck_Pria_Continuous      float64\n",
      "Keck_RMI_cdd              float64\n",
      "FP counts % inhibition    float64\n",
      "dtype: object\n",
      "(43452, 2)\n",
      "(14485, 2)\n"
     ]
    }
   ],
   "source": [
    "print train_pd.dtypes\n",
    "\n",
    "X_train, y_train = extract_feature_and_label(train_pd,\n",
    "                                             feature_name='Fingerprints',\n",
    "                                             label_name_list=['Keck_Pria_AS_Retest', 'Keck_Pria_FP_data'])\n",
    "X_test, y_test = extract_feature_and_label(test_pd,\n",
    "                                           feature_name='Fingerprints',\n",
    "                                           label_name_list=['Keck_Pria_AS_Retest', 'Keck_Pria_FP_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Single Classification with Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K40m (CNMeM is disabled, CuDNN 4004)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whole shpae  (43452, 8)\n",
      "(43452, 1)\n",
      "whole shpae  (14485, 8)\n",
      "(14485, 1)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from single_task import *\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import json\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from function import *\n",
    "\n",
    "\n",
    "def setup_model():\n",
    "    model = Sequential()\n",
    "    if batch_is_use:\n",
    "        batch_normalizer = BatchNormalization(epsilon=batch_normalizer_epsilon,\n",
    "                                              mode=batch_normalizer_mode,\n",
    "                                              axis=batch_normalizer_axis,\n",
    "                                              momentum=batch_normalizer_momentum,\n",
    "                                              weights=batch_normalizer_weights,\n",
    "                                              beta_init=batch_normalizer_beta_init,\n",
    "                                              gamma_init=batch_normalizer_gamma_init)\n",
    "    layers = conf['layers']\n",
    "    layer_number = len(layers)\n",
    "    for i in range(layer_number):\n",
    "        init = layers[i]['init']\n",
    "        activation = layers[i]['activation']\n",
    "        if i == 0:\n",
    "            hidden_units = int(layers[i]['hidden_units'])\n",
    "            dropout = float(layers[i]['dropout'])\n",
    "            model.add(Dense(hidden_units, input_dim=input_layer_dimension, init=init, activation=activation))\n",
    "            model.add(Dropout(dropout))\n",
    "        elif i == layer_number-1:\n",
    "            if batch_is_use:\n",
    "                model.add(batch_normalizer)\n",
    "            # output_layer_dimension = layers[i]['output_layer_dimension']\n",
    "            model.add(Dense(output_layer_dimension, init=init, activation=activation))\n",
    "        else:\n",
    "            hidden_units = int(layers[i]['hidden_units'])\n",
    "            dropout = float(layers[i]['dropout'])\n",
    "            model.add(Dense(hidden_units, init=init, activation=activation))\n",
    "            model.add(Dropout(dropout))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_predict(X_train, y_train, X_val, y_val, X_test, y_test, PMTNN_weight_file):\n",
    "    model = setup_model()\n",
    "    if early_stopping_option == 'auc':\n",
    "        early_stopping = KeckCallBackOnAUC(X_train, y_train, X_val, y_val, patience=early_stopping_patience)\n",
    "        callbacks = [early_stopping]\n",
    "    elif early_stopping_option == 'precision':\n",
    "        early_stopping = KeckCallBackOnPrecision(X_train, y_train, X_val, y_val, patience=early_stopping_patience)\n",
    "        callbacks = [early_stopping]\n",
    "    else:\n",
    "        callbacks = []\n",
    "\n",
    "    model.compile(loss=compile_loss, optimizer=compile_optimizer)\n",
    "    model.fit(X_train, y_train, nb_epoch=fit_nb_epoch, batch_size=fit_batch_size, verbose=fit_verbose,\n",
    "              validation_data=(X_val, y_val), callbacks=callbacks)\n",
    "    model.save_weights(PMTNN_weight_file)\n",
    "\n",
    "    if early_stopping_option == 'auc' or early_stopping_option == 'precision':\n",
    "        model = early_stopping.get_best_model()\n",
    "    y_pred_on_train = model.predict(X_train)\n",
    "    y_pred_on_val = model.predict(X_val)\n",
    "    y_pred_on_test = model.predict(X_test)\n",
    "\n",
    "    print\n",
    "    print('train precision: {}'.format(average_precision_score(y_train, y_pred_on_train)))\n",
    "    print('train auc: {}'.format(roc_auc_score(y_train, y_pred_on_train)))\n",
    "    print('validation precision: {}'.format(average_precision_score(y_val, y_pred_on_val)))\n",
    "    print('validation auc: {}'.format(roc_auc_score(y_val, y_pred_on_val)))\n",
    "    print('test precision: {}'.format(average_precision_score(y_test, y_pred_on_test)))\n",
    "    print('test auc: {}'.format(roc_auc_score(y_test, y_pred_on_test)))\n",
    "\n",
    "    for EF_ratio in EF_ratio_list:\n",
    "        n_actives, ef, ef_max = enrichment_factor_single(y_test, y_pred_on_test, EF_ratio)\n",
    "        print('ratio: {}, EF: {}, EF_max: {}\\tactive: {}'.format(EF_ratio, ef, ef_max, n_actives))\n",
    "\n",
    "        \n",
    "def get_EF_score_with_existing_model(X_test, y_test, file_path, EF_ratio):\n",
    "    model = setup_model()\n",
    "    model.load_weights(file_path)\n",
    "    y_pred_on_test = model.predict(X_test)\n",
    "    n_actives, ef = enrichment_factor(y_test, y_pred_on_test, EF_ratio)\n",
    "    print('test precision: {}'.format(average_precision_score(y_test, y_pred_on_test)))\n",
    "    print('test auc: {}'.format(roc_auc_score(y_test, y_pred_on_test)))\n",
    "    print('EF: {},\\tactive: {}'.format(ef, n_actives))    \n",
    "    return\n",
    "\n",
    "def extract_feature_and_label(data_pd,\n",
    "                              feature_name,\n",
    "                              label_name_list):\n",
    "    X_data = np.zeros(shape=(data_pd.shape[0], 1024))\n",
    "    y_data = np.zeros(shape=(data_pd.shape[0], len(label_name_list)))\n",
    "    index = 0\n",
    "    print 'whole shpae ', data_pd.shape\n",
    "    for _, row in data_pd.iterrows():\n",
    "        feature = list(row[feature_name])\n",
    "        labels = row[label_name_list]\n",
    "        X_data[index] = np.array(feature)\n",
    "        y_data[index] = np.array(labels)\n",
    "        index += 1\n",
    "    X_data = X_data.astype(float)\n",
    "    y_data = y_data.astype(int)\n",
    "\n",
    "    # In case we just train on one target\n",
    "    # y would be (n,) vector\n",
    "    # then we should change it to (n,1) 1D matrix\n",
    "    # to keep consistency\n",
    "    print y_data.shape\n",
    "    if y_data.ndim == 1:\n",
    "        n = y_data.shape[0]\n",
    "        y_data = y_data.reshape(n, 1)\n",
    "\n",
    "    return X_data, y_data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    config_json_file = '../json/classification.json'\n",
    "    PMTNN_weight_file = 'temp.h5'\n",
    "    \n",
    "    \n",
    "    with open(config_json_file, 'r') as f:\n",
    "        conf = json.load(f)\n",
    "    \n",
    "    X_train, y_train = extract_feature_and_label(train_pd,\n",
    "                                                 feature_name='Fingerprints',\n",
    "                                                 label_name_list=['Keck_Pria_AS_Retest'])\n",
    "    X_test, y_test = extract_feature_and_label(test_pd,\n",
    "                                               feature_name='Fingerprints',\n",
    "                                               label_name_list=['Keck_Pria_AS_Retest'])\n",
    "\n",
    "    cross_validation_split = StratifiedShuffleSplit(y_train, 1, test_size=0.15, random_state=1)\n",
    "    for t_index, val_index in cross_validation_split:\n",
    "        X_t, X_val = X_train[t_index], X_train[val_index]\n",
    "        y_t, y_val = y_train[t_index], y_train[val_index]\n",
    "\n",
    "    input_layer_dimension = 1024\n",
    "    output_layer_dimension = 1\n",
    "\n",
    "    print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36934, 1024)\n",
      "(36934, 1)\n",
      "(6518, 1024)\n",
      "(6518, 1)\n",
      "(14485, 1024)\n",
      "(14485, 1)\n"
     ]
    }
   ],
   "source": [
    "print X_t.shape\n",
    "print y_t.shape\n",
    "print X_val.shape\n",
    "print y_val.shape\n",
    "print X_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "Precision Train: 0.011989 ---- Precision Val: 0.001356\n",
      "AUC Train: 0.771883 ---- AUC Val: 0.558988\n",
      "\n",
      "Epoch 2/200\n",
      "Precision Train: 0.094106 ---- Precision Val: 0.003316\n",
      "AUC Train: 0.885775 ---- AUC Val: 0.633412\n",
      "\n",
      "Epoch 3/200\n",
      "Precision Train: 0.113974 ---- Precision Val: 0.003164\n",
      "AUC Train: 0.914344 ---- AUC Val: 0.694210\n",
      "\n",
      "Epoch 4/200\n",
      "Precision Train: 0.136872 ---- Precision Val: 0.003911\n",
      "AUC Train: 0.940345 ---- AUC Val: 0.648419\n",
      "\n",
      "Epoch 5/200\n",
      "Precision Train: 0.285550 ---- Precision Val: 0.005908\n",
      "AUC Train: 0.954152 ---- AUC Val: 0.725892\n",
      "\n",
      "Epoch 6/200\n",
      "Precision Train: 0.241442 ---- Precision Val: 0.009909\n",
      "AUC Train: 0.971891 ---- AUC Val: 0.749128\n",
      "\n",
      "Epoch 7/200\n",
      "Precision Train: 0.384992 ---- Precision Val: 0.004196\n",
      "AUC Train: 0.979635 ---- AUC Val: 0.660662\n",
      "\n",
      "Epoch 8/200\n",
      "Precision Train: 0.488285 ---- Precision Val: 0.046902\n",
      "AUC Train: 0.980207 ---- AUC Val: 0.796016\n",
      "\n",
      "Epoch 9/200\n",
      "Precision Train: 0.557133 ---- Precision Val: 0.004431\n",
      "AUC Train: 0.975646 ---- AUC Val: 0.752682\n",
      "\n",
      "Epoch 10/200\n",
      "Precision Train: 0.649522 ---- Precision Val: 0.007054\n",
      "AUC Train: 0.982344 ---- AUC Val: 0.673563\n",
      "\n",
      "Epoch 11/200\n",
      "Precision Train: 0.658331 ---- Precision Val: 0.003971\n",
      "AUC Train: 0.986808 ---- AUC Val: 0.726660\n",
      "\n",
      "Epoch 12/200\n",
      "Precision Train: 0.756441 ---- Precision Val: 0.012177\n",
      "AUC Train: 0.995749 ---- AUC Val: 0.736951\n",
      "\n",
      "Epoch 13/200\n",
      "Precision Train: 0.800003 ---- Precision Val: 0.009284\n",
      "AUC Train: 0.997045 ---- AUC Val: 0.757290\n",
      "\n",
      "Epoch 14/200\n",
      "Precision Train: 0.787096 ---- Precision Val: 0.006781\n",
      "AUC Train: 0.998290 ---- AUC Val: 0.838669\n",
      "\n",
      "Epoch 15/200\n",
      "Precision Train: 0.894922 ---- Precision Val: 0.017992\n",
      "AUC Train: 0.999753 ---- AUC Val: 0.805318\n",
      "\n",
      "Epoch 16/200\n",
      "Precision Train: 0.866887 ---- Precision Val: 0.021879\n",
      "AUC Train: 0.999472 ---- AUC Val: 0.680277\n",
      "\n",
      "Epoch 17/200\n",
      "Precision Train: 0.888292 ---- Precision Val: 0.009176\n",
      "AUC Train: 0.999671 ---- AUC Val: 0.662461\n",
      "\n",
      "Epoch 18/200\n",
      "Precision Train: 0.944274 ---- Precision Val: 0.034999\n",
      "AUC Train: 0.999884 ---- AUC Val: 0.731838\n",
      "\n",
      "Epoch 19/200\n",
      "Precision Train: 0.952221 ---- Precision Val: 0.030805\n",
      "AUC Train: 0.999936 ---- AUC Val: 0.692411\n",
      "\n",
      "Epoch 20/200\n",
      "Precision Train: 0.924435 ---- Precision Val: 0.009932\n",
      "AUC Train: 0.999887 ---- AUC Val: 0.742238\n",
      "\n",
      "Epoch 21/200\n",
      "Precision Train: 0.948937 ---- Precision Val: 0.016861\n",
      "AUC Train: 0.999881 ---- AUC Val: 0.772627\n",
      "\n",
      "Epoch 22/200\n",
      "Precision Train: 0.971599 ---- Precision Val: 0.019948\n",
      "AUC Train: 0.999963 ---- AUC Val: 0.797047\n",
      "\n",
      "Epoch 23/200\n",
      "Precision Train: 0.984256 ---- Precision Val: 0.022562\n",
      "AUC Train: 0.999978 ---- AUC Val: 0.807403\n",
      "\n",
      "Epoch 24/200\n",
      "Precision Train: 0.994172 ---- Precision Val: 0.056998\n",
      "AUC Train: 0.999994 ---- AUC Val: 0.725805\n",
      "\n",
      "Epoch 25/200\n",
      "Precision Train: 0.986374 ---- Precision Val: 0.053599\n",
      "AUC Train: 0.999980 ---- AUC Val: 0.723281\n",
      "\n",
      "Epoch 26/200\n",
      "Precision Train: 0.957501 ---- Precision Val: 0.007256\n",
      "AUC Train: 0.999959 ---- AUC Val: 0.798934\n",
      "\n",
      "Epoch 27/200\n",
      "Precision Train: 0.987241 ---- Precision Val: 0.018489\n",
      "AUC Train: 0.999986 ---- AUC Val: 0.774053\n",
      "\n",
      "Epoch 28/200\n",
      "Precision Train: 0.994188 ---- Precision Val: 0.032583\n",
      "AUC Train: 0.999993 ---- AUC Val: 0.826228\n",
      "\n",
      "Epoch 29/200\n",
      "Precision Train: 0.993216 ---- Precision Val: 0.044441\n",
      "AUC Train: 0.999993 ---- AUC Val: 0.800579\n",
      "\n",
      "Epoch 30/200\n",
      "Precision Train: 0.993803 ---- Precision Val: 0.078776\n",
      "AUC Train: 0.999994 ---- AUC Val: 0.851021\n",
      "\n",
      "Epoch 31/200\n",
      "Precision Train: 0.989877 ---- Precision Val: 0.041768\n",
      "AUC Train: 0.999991 ---- AUC Val: 0.776159\n",
      "\n",
      "Epoch 32/200\n",
      "Precision Train: 0.993383 ---- Precision Val: 0.077657\n",
      "AUC Train: 0.999993 ---- AUC Val: 0.778375\n",
      "\n",
      "Epoch 33/200\n",
      "Precision Train: 0.995111 ---- Precision Val: 0.046498\n",
      "AUC Train: 0.999995 ---- AUC Val: 0.726331\n",
      "\n",
      "Epoch 34/200\n",
      "Precision Train: 0.996846 ---- Precision Val: 0.123923\n",
      "AUC Train: 0.999997 ---- AUC Val: 0.772758\n",
      "\n",
      "Epoch 35/200\n",
      "Precision Train: 0.997433 ---- Precision Val: 0.106359\n",
      "AUC Train: 0.999997 ---- AUC Val: 0.736007\n",
      "\n",
      "Epoch 36/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.126720\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.709985\n",
      "\n",
      "Epoch 37/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.105023\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.759901\n",
      "\n",
      "Epoch 38/200\n",
      "Precision Train: 0.996846 ---- Precision Val: 0.159796\n",
      "AUC Train: 0.999997 ---- AUC Val: 0.835026\n",
      "\n",
      "Epoch 39/200\n",
      "Precision Train: 0.998750 ---- Precision Val: 0.114642\n",
      "AUC Train: 0.999999 ---- AUC Val: 0.769752\n",
      "\n",
      "Epoch 40/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.171715\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.785001\n",
      "\n",
      "Epoch 41/200\n",
      "Precision Train: 0.999383 ---- Precision Val: 0.148303\n",
      "AUC Train: 0.999999 ---- AUC Val: 0.781447\n",
      "\n",
      "Epoch 42/200\n",
      "Precision Train: 0.998100 ---- Precision Val: 0.237236\n",
      "AUC Train: 0.999998 ---- AUC Val: 0.792593\n",
      "\n",
      "Epoch 43/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.189872\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.717401\n",
      "\n",
      "Epoch 44/200\n",
      "Precision Train: 0.999383 ---- Precision Val: 0.223623\n",
      "AUC Train: 0.999999 ---- AUC Val: 0.704171\n",
      "\n",
      "Epoch 45/200\n",
      "Precision Train: 0.998100 ---- Precision Val: 0.274888\n",
      "AUC Train: 0.999998 ---- AUC Val: 0.730346\n",
      "\n",
      "Epoch 46/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.241039\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.731663\n",
      "\n",
      "Epoch 47/200\n",
      "Precision Train: 0.999383 ---- Precision Val: 0.297338\n",
      "AUC Train: 0.999999 ---- AUC Val: 0.724137\n",
      "\n",
      "Epoch 48/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.331673\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.738399\n",
      "\n",
      "Epoch 49/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.331685\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.774448\n",
      "\n",
      "Epoch 50/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.288671\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.723501\n",
      "\n",
      "Epoch 51/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311298\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.723413\n",
      "\n",
      "Epoch 52/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.288739\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.730237\n",
      "\n",
      "Epoch 53/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.253421\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.761239\n",
      "\n",
      "Epoch 54/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.253340\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.760954\n",
      "\n",
      "Epoch 55/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.254282\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.806920\n",
      "\n",
      "Epoch 56/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.254354\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.831779\n",
      "\n",
      "Epoch 57/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312469\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.790486\n",
      "\n",
      "Epoch 58/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265451\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.787349\n",
      "\n",
      "Epoch 59/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289329\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.782017\n",
      "\n",
      "Epoch 60/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311985\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.761612\n",
      "\n",
      "Epoch 61/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311299\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.717226\n",
      "\n",
      "Epoch 62/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265428\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.792351\n",
      "\n",
      "Epoch 63/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.275976\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.848608\n",
      "\n",
      "Epoch 64/200\n",
      "Precision Train: 0.975032 ---- Precision Val: 0.290056\n",
      "AUC Train: 0.979297 ---- AUC Val: 0.752222\n",
      "\n",
      "Epoch 65/200\n",
      "Precision Train: 0.994554 ---- Precision Val: 0.312257\n",
      "AUC Train: 0.999993 ---- AUC Val: 0.806174\n",
      "\n",
      "Epoch 66/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.335885\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.817869\n",
      "\n",
      "Epoch 67/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312622\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.778660\n",
      "\n",
      "Epoch 68/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.332053\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.753494\n",
      "\n",
      "Epoch 69/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289233\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.752331\n",
      "\n",
      "Epoch 70/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289423\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.785857\n",
      "\n",
      "Epoch 71/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311451\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.780108\n",
      "\n",
      "Epoch 72/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311481\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.762161\n",
      "\n",
      "Epoch 73/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.331667\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.769445\n",
      "\n",
      "Epoch 74/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.331561\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.744959\n",
      "\n",
      "Epoch 75/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.331572\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.742699\n",
      "\n",
      "Epoch 76/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.331775\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.784540\n",
      "\n",
      "Epoch 77/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.332018\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.781030\n",
      "\n",
      "Epoch 78/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.332008\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.791649\n",
      "\n",
      "Epoch 79/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311596\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.750225\n",
      "\n",
      "Epoch 80/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311574\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.791792\n",
      "\n",
      "Epoch 81/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289198\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.834259\n",
      "\n",
      "Epoch 82/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311599\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.813678\n",
      "\n",
      "Epoch 83/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289345\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.836672\n",
      "\n",
      "Epoch 84/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265516\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.803300\n",
      "\n",
      "Epoch 85/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265439\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.837374\n",
      "\n",
      "Epoch 86/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.266012\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.853106\n",
      "\n",
      "Epoch 87/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289467\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.809356\n",
      "\n",
      "Epoch 88/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289647\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.811616\n",
      "\n",
      "Epoch 89/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289771\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.792395\n",
      "\n",
      "Epoch 90/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289633\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.751081\n",
      "\n",
      "Epoch 91/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289178\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.778178\n",
      "\n",
      "Epoch 92/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289257\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.767273\n",
      "\n",
      "Epoch 93/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289187\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.753823\n",
      "\n",
      "Epoch 94/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289054\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.747241\n",
      "\n",
      "Epoch 95/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289387\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.719530\n",
      "\n",
      "Epoch 96/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.297584\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.711280\n",
      "\n",
      "Epoch 97/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312010\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.740637\n",
      "\n",
      "Epoch 98/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312907\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.754898\n",
      "\n",
      "Epoch 99/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312608\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.764596\n",
      "\n",
      "Epoch 100/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.313544\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.776005\n",
      "\n",
      "Epoch 101/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312649\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.776291\n",
      "\n",
      "Epoch 102/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.297972\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.758628\n",
      "\n",
      "Epoch 103/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311496\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.725476\n",
      "\n",
      "Epoch 104/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311850\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.755140\n",
      "\n",
      "Epoch 105/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289070\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.746407\n",
      "\n",
      "Epoch 106/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311967\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.755973\n",
      "\n",
      "Epoch 107/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.297635\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.746199\n",
      "\n",
      "Epoch 108/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311788\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.761744\n",
      "\n",
      "Epoch 109/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289217\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.761305\n",
      "\n",
      "Epoch 110/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289925\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.768940\n",
      "\n",
      "Epoch 111/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312489\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.755118\n",
      "\n",
      "Epoch 112/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.298912\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.795028\n",
      "\n",
      "Epoch 113/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.396753\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.823475\n",
      "\n",
      "Epoch 114/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.314881\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.793427\n",
      "\n",
      "Epoch 115/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.313831\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.800206\n",
      "\n",
      "Epoch 116/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.290739\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.783970\n",
      "\n",
      "Epoch 117/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.290758\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.766154\n",
      "\n",
      "Epoch 118/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.290614\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.769884\n",
      "\n",
      "Epoch 119/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.266694\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.771650\n",
      "\n",
      "Epoch 120/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.290128\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.783915\n",
      "\n",
      "Epoch 121/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.242605\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.779538\n",
      "\n",
      "Epoch 122/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265752\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.775731\n",
      "\n",
      "Epoch 123/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265850\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.792286\n",
      "\n",
      "Epoch 124/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265224\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.794129\n",
      "\n",
      "Epoch 125/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265136\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.777936\n",
      "\n",
      "Epoch 126/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.288967\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.769643\n",
      "\n",
      "Epoch 127/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.288987\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.748141\n",
      "\n",
      "Epoch 128/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289166\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.761184\n",
      "\n",
      "Epoch 129/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265262\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.757487\n",
      "\n",
      "Epoch 130/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265184\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.768809\n",
      "\n",
      "Epoch 131/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311352\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.748053\n",
      "\n",
      "Epoch 132/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.297325\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.714527\n",
      "\n",
      "Epoch 133/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311474\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.764223\n",
      "\n",
      "Epoch 134/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.288779\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.753999\n",
      "\n",
      "Epoch 135/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311508\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.765342\n",
      "\n",
      "Epoch 136/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311905\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.776181\n",
      "\n",
      "Epoch 137/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311624\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.739814\n",
      "\n",
      "Epoch 138/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289014\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.746385\n",
      "\n",
      "Epoch 139/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265334\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.760438\n",
      "\n",
      "Epoch 140/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311742\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.770301\n",
      "\n",
      "Epoch 141/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312522\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.784540\n",
      "\n",
      "Epoch 142/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289874\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.784299\n",
      "\n",
      "Epoch 143/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265640\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.747855\n",
      "\n",
      "Epoch 144/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.265636\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.736984\n",
      "\n",
      "Epoch 145/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.290202\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.749007\n",
      "\n",
      "Epoch 146/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.266752\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.793953\n",
      "\n",
      "Epoch 147/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.290760\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.801369\n",
      "\n",
      "Epoch 148/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.312818\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.798100\n",
      "\n",
      "Epoch 149/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.275991\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.782632\n",
      "\n",
      "Epoch 150/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.275935\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.770926\n",
      "\n",
      "Epoch 151/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311980\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.751925\n",
      "\n",
      "Epoch 152/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.313232\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.763806\n",
      "\n",
      "Epoch 153/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.266102\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.752902\n",
      "\n",
      "Epoch 154/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.240354\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.706837\n",
      "\n",
      "Epoch 155/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.250441\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.760778\n",
      "\n",
      "Epoch 156/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.275181\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.798824\n",
      "\n",
      "Epoch 157/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289927\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.827808\n",
      "\n",
      "Epoch 158/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289276\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.766011\n",
      "\n",
      "Epoch 159/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289356\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.773307\n",
      "\n",
      "Epoch 160/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289982\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.792757\n",
      "\n",
      "Epoch 161/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289704\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.795313\n",
      "\n",
      "Epoch 162/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.311913\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.787722\n",
      "\n",
      "Epoch 163/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.372955\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.782368\n",
      "\n",
      "Epoch 164/200\n",
      "Precision Train: 1.000000 ---- Precision Val: 0.289449\n",
      "AUC Train: 1.000000 ---- AUC Val: 0.785561\n",
      "\n",
      "train precision: 1.0\n",
      "train auc: 1.0\n",
      "validation precision: 0.289449260727\n",
      "validation auc: 0.785560699476\n",
      "test precision: 0.214054653644\n",
      "test auc: 0.971482134218\n",
      "ratio: 0.02, EF: 37.5, EF_max: 50.0\tactive: 16\n",
      "ratio: 0.01, EF: 68.75, EF_max: 100.0\tactive: 16\n",
      "ratio: 0.0015, EF: 250.0, EF_max: 666.666666667\tactive: 16\n",
      "ratio: 0.001, EF: 250.0, EF_max: 875.0\tactive: 16\n"
     ]
    }
   ],
   "source": [
    "PMTNN_weight_file = 'temp.h5'\n",
    "with open(config_json_file, 'r') as f:\n",
    "    conf = json.load(f)\n",
    "early_stopping_patience = conf['fitting']['early_stopping']['patience']\n",
    "early_stopping_option = conf['fitting']['early_stopping']['option']\n",
    "\n",
    "fit_nb_epoch = conf['fitting']['nb_epoch']\n",
    "fit_batch_size = conf['fitting']['batch_size']\n",
    "fit_verbose = conf['fitting']['verbose']\n",
    "\n",
    "compile_loss = conf['compile']['loss']\n",
    "compile_optimizer_option = conf['compile']['optimizer']['option']\n",
    "if compile_optimizer_option == 'sgd':\n",
    "    sgd_lr = conf['compile']['optimizer']['sgd']['lr']\n",
    "    sgd_momentum = conf['compile']['optimizer']['sgd']['momentum']\n",
    "    sgd_decay = conf['compile']['optimizer']['sgd']['decay']\n",
    "    sgd_nestrov = conf['compile']['optimizer']['sgd']['nestrov']\n",
    "    compile_optimizer = SGD(lr=sgd_lr, momentum=sgd_momentum, decay=sgd_decay, nesterov=sgd_nestrov)\n",
    "else:\n",
    "    adam_lr = conf['compile']['optimizer']['adam']['lr']\n",
    "    adam_beta_1 = conf['compile']['optimizer']['adam']['beta_1']\n",
    "    adam_beta_2 = conf['compile']['optimizer']['adam']['beta_2']\n",
    "    adam_epsilon = conf['compile']['optimizer']['adam']['epsilon']\n",
    "    compile_optimizer = Adam(lr=adam_lr, beta_1=adam_beta_1, beta_2=adam_beta_2, epsilon=adam_epsilon)\n",
    "\n",
    "batch_is_use = conf['batch']['is_use']\n",
    "if batch_is_use:\n",
    "    batch_normalizer_epsilon = conf['batch']['epsilon']\n",
    "    batch_normalizer_mode = conf['batch']['mode']\n",
    "    batch_normalizer_axis = conf['batch']['axis']\n",
    "    batch_normalizer_momentum = conf['batch']['momentum']\n",
    "    batch_normalizer_weights = conf['batch']['weights']\n",
    "    batch_normalizer_beta_init = conf['batch']['beta_init']\n",
    "    batch_normalizer_gamma_init = conf['batch']['gamma_init']\n",
    "\n",
    "EF_ratio_list = conf['enrichment_factor']['ratio_list']\n",
    "\n",
    "train_and_predict(X_t, y_t, X_val, y_val, X_test, y_test, PMTNN_weight_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot EF and EF-Max graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (25.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABbIAAAJoCAYAAACz5xdKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cfd2802d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_EF_score_with_existing_model(X_test, y_test, file_path, EF_ratio=0.1):\n",
    "    model = setup_model()\n",
    "    model.load_weights(file_path)\n",
    "    y_pred_on_test = model.predict(X_test)\n",
    "    EF_ratio_list = [0.1, 0.5, 1, 2, 3, 4, 5, 10, 15, 20]\n",
    "    EF_ratio_list = np.array(EF_ratio_list) / 100.0\n",
    "    \n",
    "    ef_values = []\n",
    "    ef_max_values = []\n",
    "    for EF_ratio in EF_ratio_list:\n",
    "        n_actives, ef, ef_max = enrichment_factor_single(y_test, y_pred_on_test, EF_ratio)\n",
    "        ef_values.append(ef)\n",
    "        ef_max_values.append(ef_max)\n",
    "    \n",
    "    x_axis = EF_ratio_list\n",
    "    y_axis = np.array(ef_values)\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    \n",
    "    x_axis = EF_ratio_list\n",
    "    y_axis = np.array(ef_max_values)\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    \n",
    "    plt.legend(['EF', 'EF Max'])\n",
    "    \n",
    "    plt.xticks(EF_ratio_list, [str(val*100) for val in EF_ratio_list])\n",
    "    plt.xlabel('Enrichment Factor Percentile (%)')\n",
    "    plt.ylabel('Enrichment Factor')\n",
    "    plt.title('EF Curve')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_EF_score_with_existing_model(X_test, y_test, PMTNN_weight_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
